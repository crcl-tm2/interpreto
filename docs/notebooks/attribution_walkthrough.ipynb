{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e4e4d1",
   "metadata": {},
   "source": [
    "# Interpreto attribution tutorial\n",
    "\n",
    "This notebook contains examples of what you can do with the attribution module of Interpreto.\n",
    "\n",
    "The first part focuses on classification, while the second explores generation.\n",
    "\n",
    "*author: Fanny Jourdan & Antonin PochÃ©*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8a9bf",
   "metadata": {},
   "source": [
    "### Available methods\n",
    "\n",
    "All methods will have at list one example in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d0bff",
   "metadata": {},
   "source": [
    "Inference based methods:\n",
    "- Occlusion\n",
    "- LIME\n",
    "- KernelSHAP\n",
    "- Sobol\n",
    "\n",
    "\n",
    "Gradients based methods:\n",
    "- Saliency\n",
    "- Integrated Gradients\n",
    "- SmoothGrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85a15b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6618a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a779fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from interpreto import (\n",
    "    AttributionVisualization,\n",
    "    IntegratedGradients,\n",
    "    KernelShap,\n",
    "    Lime,\n",
    "    Occlusion,\n",
    "    Saliency,\n",
    "    SmoothGrad,\n",
    "    Sobol,\n",
    "    Granularity,\n",
    ")\n",
    "from interpreto.commons.granularity import GranularityMethodAggregation\n",
    "from interpreto.attributions import InferenceModes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d54127",
   "metadata": {},
   "source": [
    "## I. Classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ca84d",
   "metadata": {},
   "source": [
    "### I.0 Setup\n",
    "\n",
    "Loading a BERT model for the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55847d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dico_name_classes = {0: \"negative\", 1: \"positive\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c531741",
   "metadata": {},
   "source": [
    "### I.1 Minimal example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is the best movie I have ever seen. The cinematography was uncharacteristically breathtaking.\"\n",
    "\n",
    "# Instantiate the Occlusion explainer with the model and tokenizer\n",
    "explainer = Occlusion(model, tokenizer)\n",
    "\n",
    "# Compute the attributions on a given sentence\n",
    "attributions = explainer(sentence)\n",
    "\n",
    "# Visualize the attributions\n",
    "AttributionVisualization(attributions[0]).display()\n",
    "# AttributionVisualization(attributions[0], class_names={0: \"Class A\", 1: \"Class B\"}).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f64a0",
   "metadata": {},
   "source": [
    "### I.2 Compute explanations on multiple inputs optimally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient-based methods have the exact same API\n",
    "explainer = Saliency(model, tokenizer)\n",
    "\n",
    "# Inputs can also be a list of strings, or even `input_ids`\n",
    "attributions = explainer(\n",
    "    [\n",
    "        \"This is the best movie I have ever seen.\",\n",
    "        \"I hate this movie.\",\n",
    "        \"This movie is super good. I love it.\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# To visualize multiple attributions, simply loop over the list of attributions\n",
    "for attr in attributions:\n",
    "    AttributionVisualization(attr, class_names=dico_name_classes).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a40b2",
   "metadata": {},
   "source": [
    "### I.3 Changing the `granularity_level` and `inference_mode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cc361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify the default parameters of the Lime explainer\n",
    "explainer = Lime(\n",
    "    # ---------------------------------\n",
    "    # common to all attribution methods\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=32,  # default is 4\n",
    "    # study perturbations impact on the softmax of the logits, default is the logits\n",
    "    inference_mode=InferenceModes.SOFTMAX,\n",
    "    # ----------------------------------------\n",
    "    # common to all perturbation-based methods\n",
    "    n_perturbations=20,\n",
    "    # attribution at the word level, default is the token level\n",
    "    granularity=Granularity.WORD,\n",
    "    # ----------------\n",
    "    # specific to Lime\n",
    "    # arguments possible value are in classes static attributes, in Enums\n",
    "    distance_function=Lime.distance_functions.HAMMING,\n",
    ")\n",
    "\n",
    "# The `__call__` method is a renaming of the `explain` method\n",
    "attrs = explainer.explain(model_inputs=\"Would Interpreto be a good movie name?\")\n",
    "\n",
    "# Let's visualize the attributions\n",
    "AttributionVisualization(attrs[0], class_names=dico_name_classes).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f69c68",
   "metadata": {},
   "source": [
    "### I.4 Explaining multiple classes at once\n",
    "\n",
    "Specifying what to explain is the role of the `targets` argument of the `explain` method.\n",
    "\n",
    "For `n` inputs, there should be `n` elements in the `targets` argument.\n",
    "But each elements of the `targets` can specify several classes to explain.\n",
    "Therefore, the `targets` shape should be `(n, t)`, with `t` the number of classes to explain.\n",
    "\n",
    "When `targets` are not specified, the explainer compute the model's prediction for each input.\n",
    "Then it explains the model's prediction for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember, the BERT trained on IMDB, movie reviews\n",
    "explainer = KernelShap(model, tokenizer)\n",
    "\n",
    "# we explain the prediction for both the positive and negative class\n",
    "attributions = explainer(\n",
    "    model_inputs=[\"I do not know if this is the best or the worst movie ever, I am confused.\"],\n",
    "    targets=torch.tensor([[0, 1]]),\n",
    ")\n",
    "\n",
    "# be careful, we use a new visualization class for multi class attributions\n",
    "AttributionVisualization(attributions[0], class_names=dico_name_classes).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd089c0e",
   "metadata": {},
   "source": [
    "## II. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf35d7",
   "metadata": {},
   "source": [
    "### II.0 Setup\n",
    "\n",
    "Let's load a good old GPT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "361aadfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35222a12",
   "metadata": {},
   "source": [
    "### II.1 Minimal example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa87bb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m SmoothGrad(model, tokenizer, granularity\u001b[38;5;241m=\u001b[39mGranularity\u001b[38;5;241m.\u001b[39mALL_TOKENS)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# if no target is specified, we generate the text on our part\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# `generation_kwargs` are optional\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m attributions \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRoses are red, the sky is\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# there is a third visualization class for generation attributions\u001b[39;00m\n\u001b[1;32m     10\u001b[0m AttributionVisualization(attributions[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mdisplay()\n",
      "File \u001b[0;32m~/dev/interpreto/docs/notebooks/../../interpreto/attributions/base.py:447\u001b[0m, in \u001b[0;36mAttributionExplainer.__call__\u001b[0;34m(self, model_inputs, targets, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs: ModelInputs, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[AttributionOutput]:\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    Enables the explainer instance to be called as a function.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m        List[AttributionOutput]: A list of attribution outputs, one per input sample.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/interpreto/docs/notebooks/../../interpreto/attributions/base.py:369\u001b[0m, in \u001b[0;36mAttributionExplainer.explain\u001b[0;34m(self, model_inputs, targets, **model_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m model_inputs_to_explain: Iterable[TensorMapping]\n\u001b[1;32m    368\u001b[0m sanitized_targets: Iterable[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn t\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m--> 369\u001b[0m model_inputs_to_explain, sanitized_targets_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_inputs_to_explain_and_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43msanitized_model_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m sanitized_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sanitized_targets_gen)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Create perturbation masks and perturb inputs based on the masks.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# Inputs might be embedded during the perturbation process if the perturbator works with embeddings.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/dev/interpreto/.venv/lib/python3.12/site-packages/jaxtyping/_decorator.py:483\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn_impl\u001b[0;34m(args, kwargs, bound, memos)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# Actually call the function.\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_signature\u001b[38;5;241m.\u001b[39mreturn_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mSignature\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# Now type-check the return value. We need to include the\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;66;03m# parameters in the type-checking here in case there are any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# checking of the parameters. Unfortunately there doesn't seem\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# to be a way around that, so c'est la vie.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     kwargs[output_name] \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m~/dev/interpreto/docs/notebooks/../../interpreto/attributions/base.py:663\u001b[0m, in \u001b[0;36mGenerationAttributionExplainer.process_inputs_to_explain_and_targets\u001b[0;34m(self, model_inputs, targets, **model_kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m sanitized_targets: \u001b[38;5;28mlist\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 663\u001b[0m     model_inputs_to_explain, sanitized_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_inputs_to_explain_and_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     sanitized_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_targets(targets)\n",
      "File \u001b[0;32m/usr/lib/python3.12/functools.py:946\u001b[0m, in \u001b[0;36msingledispatchmethod.__get__.<locals>._method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_method\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    945\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher\u001b[38;5;241m.\u001b[39mdispatch(args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m--> 946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/interpreto/docs/notebooks/../../interpreto/model_wrapping/generation_inference_wrapper.py:107\u001b[0m, in \u001b[0;36mGenerationInferenceWrapper._\u001b[0;34m(self, model_inputs, **generation_kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m l_full_mappings, l_targets_ids \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_input \u001b[38;5;129;01min\u001b[39;00m model_inputs:\n\u001b[0;32m--> 107\u001b[0m     full_mappings, targets_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_inputs_to_explain_and_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     l_full_mappings\u001b[38;5;241m.\u001b[39mappend(full_mappings)\n\u001b[1;32m    109\u001b[0m     l_targets_ids\u001b[38;5;241m.\u001b[39mappend(targets_ids)\n",
      "File \u001b[0;32m/usr/lib/python3.12/functools.py:946\u001b[0m, in \u001b[0;36msingledispatchmethod.__get__.<locals>._method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_method\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    945\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher\u001b[38;5;241m.\u001b[39mdispatch(args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m--> 946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/interpreto/docs/notebooks/../../interpreto/model_wrapping/generation_inference_wrapper.py:80\u001b[0m, in \u001b[0;36mGenerationInferenceWrapper._\u001b[0;34m(self, model_inputs, **generation_kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@get_inputs_to_explain_and_targets\u001b[39m\u001b[38;5;241m.\u001b[39mregister(MutableMapping)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs: TensorMapping, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[TensorMapping, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate continuations for a batch of sequences.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m        targets_ids (torch.Tensor): The token IDs of the generated part.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     filtered_model_inputs \u001b[38;5;241m=\u001b[39m {key: \u001b[43mmodel_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m     82\u001b[0m     full_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered_model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs)\n\u001b[1;32m     83\u001b[0m     original_length \u001b[38;5;241m=\u001b[39m model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# the API is the same as classification\n",
    "\n",
    "explainer = SmoothGrad(model, tokenizer, granularity=Granularity.ALL_TOKENS)\n",
    "\n",
    "# if no target is specified, we generate the text on our part\n",
    "# `generation_kwargs` are optional\n",
    "attributions = explainer(\"Roses are red, the sky is\", max_length=16)\n",
    "\n",
    "# there is a third visualization class for generation attributions\n",
    "AttributionVisualization(attributions[0]).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958c644",
   "metadata": {},
   "source": [
    "## II.2 Explain your own outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naturally gradient-based methods also work\n",
    "explainer = IntegratedGradients(model, tokenizer)\n",
    "\n",
    "# you can pass strings as targets and even several samples at once\n",
    "attributions = explainer(\n",
    "    model_inputs=[\"Interpreto can explain\", \"And even treat\"],\n",
    "    targets=[\" the outputs you provide.\", \" several samples at once.\"],\n",
    ")\n",
    "\n",
    "# for multiple samples, visualization need to be done one by one\n",
    "for attr in attributions:\n",
    "    AttributionVisualization(attr).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb50ee7",
   "metadata": {},
   "source": [
    "### II.3 Explain from tokenized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default granularity ignores the special tokens\n",
    "# but we can set it to ALL_TOKENS to include them\n",
    "explainer = Sobol(model, tokenizer, granularity=Granularity.ALL_TOKENS)\n",
    "\n",
    "tokenized_inputs = tokenizer(\"Hi there, how are you?\", return_tensors=\"pt\")\n",
    "tokenized_targets = tokenizer(\"I am fine, thank you!\", return_tensors=\"pt\")\n",
    "\n",
    "# these inputs/targets can be passed just like the others\n",
    "attributions = explainer(tokenized_inputs, tokenized_targets)\n",
    "\n",
    "AttributionVisualization(attributions[0]).display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpreto",
   "language": "python",
   "name": "interpreto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
