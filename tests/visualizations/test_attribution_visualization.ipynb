{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Visualization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from interpreto.attributions.base import AttributionOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mono Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of a attribution visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributions (1 classe)\n",
    "sentence = [\"A\", \"B\", \"C\", \"one\", \"two\", \"three\"]\n",
    "\n",
    "# Simulate attributions for a single class classification task\n",
    "attributions = torch.linspace(-10, 30, steps=len(sentence))\n",
    "single_class_classification_output = AttributionOutput(elements=sentence, attributions=attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default display\n",
    "from interpreto.visualizations.attributions.classification_highlight import SingleClassAttributionVisualization\n",
    "\n",
    "viz = SingleClassAttributionVisualization(attribution_output=single_class_classification_output)\n",
    "viz.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the border\n",
    "viz = SingleClassAttributionVisualization(attribution_output=single_class_classification_output, highlight_border=True)\n",
    "viz.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the normalization\n",
    "viz = SingleClassAttributionVisualization(attribution_output=single_class_classification_output, normalize=False)\n",
    "viz.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more space between the words\n",
    "viz = SingleClassAttributionVisualization(attribution_output=single_class_classification_output, margin_right=\"0.85em\")\n",
    "viz.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to save the results of the visualization as an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz.save(\"attributions_monoclass.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion Test on BERT outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete test on result from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from interpreto.attributions.methods.occlusion import Occlusion\n",
    "from interpreto.attributions.perturbations.base import Granularity\n",
    "from interpreto.model_wrapping.classification_inference_wrapper import ClassificationInferenceWrapper\n",
    "from interpreto.visualizations.attributions.classification_highlight import SingleClassAttributionVisualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "test_sentences = [\"Best movie ever\", \"Worst movie ever verylongword\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "inference_wrapper = ClassificationInferenceWrapper(model=model, batch_size=4)\n",
    "exp = Occlusion(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # inference_wrapper=inference_wrapper,\n",
    "    batch_size=4,\n",
    "    granularity=Granularity.WORD,\n",
    ")\n",
    "explaination = exp.explain(test_sentences)\n",
    "\n",
    "for elem in explaination:\n",
    "    print(elem.attributions, elem.elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence_explained in explaination:\n",
    "    viz = SingleClassAttributionVisualization(attribution_output=sentence_explained)\n",
    "    viz.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more space between the words\n",
    "for sentence_explained in explaination:\n",
    "    viz = SingleClassAttributionVisualization(attribution_output=sentence_explained, margin_right=\"0.85em\")\n",
    "    viz.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributions (2 classes)\n",
    "nb_classes = 2\n",
    "inputs_sentences = [\"A\", \"B\", \"C\", \"one\", \"two\", \"three\"]\n",
    "\n",
    "# Simulate and attribution output for the 1st sentence\n",
    "sentence = inputs_sentences\n",
    "# attributions = torch.rand(nb_classes, len(sentence)) # (c, l)\n",
    "attributions = torch.tensor([[0.1, 0.2, -0.3, -0.4, 0.5, 1.0], [0.6, 0.5, 0.4, 0.3, 0.2, -1]])\n",
    "attribution_output = AttributionOutput(elements=sentence, attributions=attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default display for the 1st sentence\n",
    "from interpreto.visualizations.attributions.classification_highlight import MultiClassAttributionVisualization\n",
    "\n",
    "viz = MultiClassAttributionVisualization(attribution_output=attribution_output, class_names=[\"class 1\", \"class 2\"])\n",
    "viz.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more space between the words\n",
    "viz = MultiClassAttributionVisualization(\n",
    "    attribution_output=attribution_output,\n",
    "    class_names=[\"class 1\", \"class 2\"],\n",
    "    margin_right=\"0.85em\",\n",
    ")\n",
    "viz.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an explainer for a pre-trained model (e.g., GPT-2)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "explainer = Occlusion(model=model, batch_size=4, tokenizer=tokenizer, granularity=Granularity.ALL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain outputs generated from an input sentence\n",
    "attribution_outputs = explainer.explain(model_inputs=\"Hi there, how are you?\", generation_kwargs={\"max_length\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"attribution tensor shape for the 1st sentence:\", attribution_outputs[0].attributions.shape)\n",
    "# attributions: 20x27\n",
    "# 20 output tokens , 27 attribution value for each one (input + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attribution results\n",
    "from interpreto.visualizations.attributions.classification_highlight import GenerationAttributionVisualization\n",
    "\n",
    "viz = GenerationAttributionVisualization(attribution_output=attribution_outputs[0])\n",
    "viz.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more space between the words\n",
    "viz = GenerationAttributionVisualization(attribution_output=attribution_outputs[0], margin_right=\"1em\")\n",
    "viz.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
